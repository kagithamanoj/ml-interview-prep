{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ“Š EDA & Feature Engineering â€” Interview Patterns\n",
                "\n",
                "This notebook covers common EDA and feature engineering techniques tested in ML interviews.\n",
                "\n",
                "**Topics:**\n",
                "- Data loading and inspection\n",
                "- Handling missing values\n",
                "- Feature scaling (StandardScaler, MinMax)\n",
                "- Encoding categorical variables\n",
                "- Feature selection\n",
                "- Cross-validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "np.random.seed(42)\n",
                "print('âœ… Setup complete')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Generate Synthetic Dataset\n",
                "\n",
                "We'll create a realistic dataset with mixed types, missing values, and outliers."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "n = 500\n",
                "\n",
                "# Numeric features\n",
                "age = np.random.normal(35, 10, n).clip(18, 70).astype(int)\n",
                "income = age * 1200 + np.random.normal(0, 10000, n)\n",
                "credit_score = np.random.normal(700, 50, n).clip(300, 850).astype(int)\n",
                "\n",
                "# Categorical features\n",
                "education = np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], n, p=[0.3, 0.4, 0.2, 0.1])\n",
                "employment = np.random.choice(['Employed', 'Self-Employed', 'Unemployed'], n, p=[0.7, 0.2, 0.1])\n",
                "\n",
                "# Target (binary)\n",
                "prob = 1 / (1 + np.exp(-(income/30000 + credit_score/200 - 6 + np.random.normal(0, 0.5, n))))\n",
                "approved = (np.random.random(n) < prob).astype(int)\n",
                "\n",
                "# Inject missing values (5%)\n",
                "income_with_na = income.copy().astype(float)\n",
                "income_with_na[np.random.choice(n, 25, replace=False)] = np.nan\n",
                "credit_with_na = credit_score.copy().astype(float)\n",
                "credit_with_na[np.random.choice(n, 15, replace=False)] = np.nan\n",
                "\n",
                "print(f'Dataset: {n} samples')\n",
                "print(f'Missing income: {np.isnan(income_with_na).sum()}')\n",
                "print(f'Missing credit: {np.isnan(credit_with_na).sum()}')\n",
                "print(f'Approval rate: {approved.mean():.1%}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Handling Missing Values\n",
                "\n",
                "Three approaches: **mean**, **median**, and **indicator variable**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def impute_with_indicator(arr, strategy='median'):\n",
                "    \"\"\"Impute missing values and create a 'was_missing' indicator.\"\"\"\n",
                "    is_missing = np.isnan(arr).astype(float)\n",
                "    \n",
                "    if strategy == 'mean':\n",
                "        fill_value = np.nanmean(arr)\n",
                "    elif strategy == 'median':\n",
                "        fill_value = np.nanmedian(arr)\n",
                "    else:\n",
                "        raise ValueError(f'Unknown strategy: {strategy}')\n",
                "    \n",
                "    imputed = np.where(np.isnan(arr), fill_value, arr)\n",
                "    return imputed, is_missing\n",
                "\n",
                "income_clean, income_missing_flag = impute_with_indicator(income_with_na, 'median')\n",
                "credit_clean, credit_missing_flag = impute_with_indicator(credit_with_na, 'median')\n",
                "\n",
                "print(f'Income imputed with median: {np.nanmedian(income_with_na):.0f}')\n",
                "print(f'Credit imputed with median: {np.nanmedian(credit_with_na):.0f}')\n",
                "print(f'Remaining NaN: {np.isnan(income_clean).sum() + np.isnan(credit_clean).sum()}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Feature Scaling\n",
                "\n",
                "**StandardScaler**: `z = (x - Î¼) / Ïƒ` â†’ mean=0, std=1  \n",
                "**MinMax**: `z = (x - min) / (max - min)` â†’ range [0, 1]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class StandardScaler:\n",
                "    def fit(self, X):\n",
                "        self.mean_ = np.mean(X, axis=0)\n",
                "        self.std_ = np.std(X, axis=0)\n",
                "        return self\n",
                "    \n",
                "    def transform(self, X):\n",
                "        return (X - self.mean_) / (self.std_ + 1e-8)\n",
                "    \n",
                "    def fit_transform(self, X):\n",
                "        return self.fit(X).transform(X)\n",
                "\n",
                "class MinMaxScaler:\n",
                "    def fit(self, X):\n",
                "        self.min_ = np.min(X, axis=0)\n",
                "        self.max_ = np.max(X, axis=0)\n",
                "        return self\n",
                "    \n",
                "    def transform(self, X):\n",
                "        return (X - self.min_) / (self.max_ - self.min_ + 1e-8)\n",
                "    \n",
                "    def fit_transform(self, X):\n",
                "        return self.fit(X).transform(X)\n",
                "\n",
                "# Compare\n",
                "X_numeric = np.column_stack([age, income_clean, credit_clean])\n",
                "labels = ['Age', 'Income', 'Credit Score']\n",
                "\n",
                "X_standard = StandardScaler().fit_transform(X_numeric)\n",
                "X_minmax = MinMaxScaler().fit_transform(X_numeric)\n",
                "\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
                "for i, (title, data) in enumerate([('Original', X_numeric), ('StandardScaler', X_standard), ('MinMaxScaler', X_minmax)]):\n",
                "    for j, label in enumerate(labels):\n",
                "        axes[i].hist(data[:, j], bins=30, alpha=0.6, label=label)\n",
                "    axes[i].set_title(title)\n",
                "    axes[i].legend(fontsize=8)\n",
                "    axes[i].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Encoding Categorical Variables\n",
                "\n",
                "**One-Hot Encoding** for nominal categories, **Ordinal Encoding** for ordered categories."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def one_hot_encode(arr):\n",
                "    \"\"\"One-hot encode a categorical array.\"\"\"\n",
                "    categories = sorted(set(arr))\n",
                "    encoded = np.zeros((len(arr), len(categories)))\n",
                "    cat_to_idx = {cat: i for i, cat in enumerate(categories)}\n",
                "    for i, val in enumerate(arr):\n",
                "        encoded[i, cat_to_idx[val]] = 1\n",
                "    return encoded, categories\n",
                "\n",
                "def ordinal_encode(arr, order):\n",
                "    \"\"\"Ordinal encode with specified order.\"\"\"\n",
                "    mapping = {cat: i for i, cat in enumerate(order)}\n",
                "    return np.array([mapping[val] for val in arr]).reshape(-1, 1)\n",
                "\n",
                "# One-hot for employment\n",
                "emp_encoded, emp_cats = one_hot_encode(employment)\n",
                "print(f'Employment one-hot: {emp_cats}')\n",
                "print(f'Shape: {emp_encoded.shape}')\n",
                "print(f'First 3 rows:\\n{emp_encoded[:3]}\\n')\n",
                "\n",
                "# Ordinal for education\n",
                "edu_order = ['High School', 'Bachelor', 'Master', 'PhD']\n",
                "edu_encoded = ordinal_encode(education, edu_order)\n",
                "print(f'Education ordinal: {edu_order} â†’ [0, 1, 2, 3]')\n",
                "print(f'First 5: {edu_encoded[:5].ravel()}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. K-Fold Cross-Validation from Scratch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def k_fold_split(n_samples, k=5, shuffle=True):\n",
                "    \"\"\"Generate k-fold train/val splits.\"\"\"\n",
                "    indices = np.arange(n_samples)\n",
                "    if shuffle:\n",
                "        np.random.shuffle(indices)\n",
                "    \n",
                "    fold_size = n_samples // k\n",
                "    folds = []\n",
                "    \n",
                "    for i in range(k):\n",
                "        val_start = i * fold_size\n",
                "        val_end = val_start + fold_size if i < k - 1 else n_samples\n",
                "        val_idx = indices[val_start:val_end]\n",
                "        train_idx = np.concatenate([indices[:val_start], indices[val_end:]])\n",
                "        folds.append((train_idx, val_idx))\n",
                "    \n",
                "    return folds\n",
                "\n",
                "# Demo: simple accuracy per fold using nearest-centroid\n",
                "X_full = np.column_stack([X_standard, edu_encoded, emp_encoded, income_missing_flag, credit_missing_flag])\n",
                "y_full = approved\n",
                "\n",
                "folds = k_fold_split(len(X_full), k=5)\n",
                "fold_accs = []\n",
                "\n",
                "for i, (train_idx, val_idx) in enumerate(folds):\n",
                "    # Simple nearest-centroid classifier\n",
                "    X_tr, y_tr = X_full[train_idx], y_full[train_idx]\n",
                "    X_val, y_val = X_full[val_idx], y_full[val_idx]\n",
                "    \n",
                "    c0 = X_tr[y_tr == 0].mean(axis=0)\n",
                "    c1 = X_tr[y_tr == 1].mean(axis=0)\n",
                "    \n",
                "    d0 = np.linalg.norm(X_val - c0, axis=1)\n",
                "    d1 = np.linalg.norm(X_val - c1, axis=1)\n",
                "    preds = (d1 < d0).astype(int)\n",
                "    \n",
                "    acc = np.mean(preds == y_val)\n",
                "    fold_accs.append(acc)\n",
                "    print(f'Fold {i+1}: Accuracy = {acc:.2%} (train={len(train_idx)}, val={len(val_idx)})')\n",
                "\n",
                "print(f'\\nðŸ“Š Mean CV Accuracy: {np.mean(fold_accs):.2%} Â± {np.std(fold_accs):.2%}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ’¡ Key Takeaways\n",
                "\n",
                "| Technique | When to Use | Interview Tip |\n",
                "|-----------|------------|---------------|\n",
                "| **Mean/Median Imputation** | Small % of missing values | Always mention adding a missing indicator |\n",
                "| **StandardScaler** | SVM, Logistic Regression, Neural Nets | Say \"centers data, unit variance\" |\n",
                "| **MinMaxScaler** | When you need bounded range [0,1] | Works well for image pixels, distances |\n",
                "| **One-Hot Encoding** | Nominal categories (no order) | Drop one column to avoid multicollinearity |\n",
                "| **Ordinal Encoding** | Ordered categories | Preserves natural ordering |\n",
                "| **K-Fold CV** | Model evaluation | Always use stratified for imbalanced data |"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}