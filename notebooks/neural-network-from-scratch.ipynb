{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ§® Neural Network from Scratch\n",
                "\n",
                "Building a complete neural network with forward pass, backpropagation, and training â€” using only NumPy.\n",
                "\n",
                "**What you'll learn:**\n",
                "- How neurons compute outputs\n",
                "- Forward propagation step by step\n",
                "- Loss functions and why they matter\n",
                "- Backpropagation (gradient computation)\n",
                "- Gradient descent optimization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "np.random.seed(42)\n",
                "print('âœ… Setup complete')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Activation Functions\n",
                "\n",
                "Activation functions introduce non-linearity. Without them, stacking layers would be equivalent to a single linear transformation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def sigmoid(z):\n",
                "    \"\"\"Ïƒ(z) = 1 / (1 + e^(-z))\"\"\"\n",
                "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
                "\n",
                "def sigmoid_derivative(z):\n",
                "    \"\"\"Ïƒ'(z) = Ïƒ(z) Ã— (1 - Ïƒ(z))\"\"\"\n",
                "    s = sigmoid(z)\n",
                "    return s * (1 - s)\n",
                "\n",
                "def relu(z):\n",
                "    \"\"\"ReLU(z) = max(0, z)\"\"\"\n",
                "    return np.maximum(0, z)\n",
                "\n",
                "def relu_derivative(z):\n",
                "    \"\"\"ReLU'(z) = 1 if z > 0, else 0\"\"\"\n",
                "    return (z > 0).astype(float)\n",
                "\n",
                "# Visualize\n",
                "z = np.linspace(-5, 5, 200)\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
                "\n",
                "axes[0].plot(z, sigmoid(z), 'b-', linewidth=2, label='sigmoid')\n",
                "axes[0].plot(z, sigmoid_derivative(z), 'r--', linewidth=2, label=\"sigmoid'\")\n",
                "axes[0].set_title('Sigmoid & Derivative')\n",
                "axes[0].legend()\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "axes[1].plot(z, relu(z), 'b-', linewidth=2, label='ReLU')\n",
                "axes[1].plot(z, relu_derivative(z), 'r--', linewidth=2, label=\"ReLU'\")\n",
                "axes[1].set_title('ReLU & Derivative')\n",
                "axes[1].legend()\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Generate Data\n",
                "\n",
                "We'll create a simple classification problem â€” two concentric circles."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def make_circles(n=300, noise=0.1):\n",
                "    \"\"\"Generate two concentric circles for binary classification.\"\"\"\n",
                "    t = np.random.uniform(0, 2 * np.pi, n)\n",
                "    \n",
                "    # Inner circle (class 0)\n",
                "    r_inner = 1 + np.random.randn(n // 2) * noise\n",
                "    X_inner = np.column_stack([r_inner * np.cos(t[:n//2]), r_inner * np.sin(t[:n//2])])\n",
                "    \n",
                "    # Outer circle (class 1)\n",
                "    r_outer = 3 + np.random.randn(n // 2) * noise\n",
                "    X_outer = np.column_stack([r_outer * np.cos(t[n//2:]), r_outer * np.sin(t[n//2:])])\n",
                "    \n",
                "    X = np.vstack([X_inner, X_outer])\n",
                "    y = np.array([0] * (n // 2) + [1] * (n // 2)).reshape(-1, 1)\n",
                "    \n",
                "    # Shuffle\n",
                "    idx = np.random.permutation(n)\n",
                "    return X[idx], y[idx]\n",
                "\n",
                "X, y = make_circles(400, noise=0.15)\n",
                "\n",
                "plt.figure(figsize=(6, 6))\n",
                "plt.scatter(X[y.ravel()==0, 0], X[y.ravel()==0, 1], c='blue', alpha=0.5, label='Class 0')\n",
                "plt.scatter(X[y.ravel()==1, 0], X[y.ravel()==1, 1], c='red', alpha=0.5, label='Class 1')\n",
                "plt.title('Training Data â€” Two Circles')\n",
                "plt.legend()\n",
                "plt.axis('equal')\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()\n",
                "print(f'X shape: {X.shape}, y shape: {y.shape}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Build the Neural Network\n",
                "\n",
                "Architecture: `Input(2) â†’ Hidden(16, ReLU) â†’ Hidden(8, ReLU) â†’ Output(1, Sigmoid)`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class NeuralNetwork:\n",
                "    def __init__(self, layer_sizes):\n",
                "        \"\"\"Initialize weights with He initialization.\"\"\"\n",
                "        self.weights = []\n",
                "        self.biases = []\n",
                "        \n",
                "        for i in range(len(layer_sizes) - 1):\n",
                "            # He initialization: scale by sqrt(2/fan_in)\n",
                "            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2 / layer_sizes[i])\n",
                "            b = np.zeros((1, layer_sizes[i+1]))\n",
                "            self.weights.append(w)\n",
                "            self.biases.append(b)\n",
                "        \n",
                "        self.n_layers = len(self.weights)\n",
                "        print(f'Network: {\", \".join(str(s) for s in layer_sizes)}')\n",
                "        print(f'Total parameters: {sum(w.size + b.size for w, b in zip(self.weights, self.biases)):,}')\n",
                "    \n",
                "    def forward(self, X):\n",
                "        \"\"\"Forward pass â€” store activations for backprop.\"\"\"\n",
                "        self.activations = [X]\n",
                "        self.z_values = []\n",
                "        \n",
                "        for i in range(self.n_layers):\n",
                "            z = self.activations[-1] @ self.weights[i] + self.biases[i]\n",
                "            self.z_values.append(z)\n",
                "            \n",
                "            # ReLU for hidden layers, sigmoid for output\n",
                "            if i < self.n_layers - 1:\n",
                "                a = relu(z)\n",
                "            else:\n",
                "                a = sigmoid(z)\n",
                "            self.activations.append(a)\n",
                "        \n",
                "        return self.activations[-1]\n",
                "    \n",
                "    def compute_loss(self, y_pred, y_true):\n",
                "        \"\"\"Binary cross-entropy loss.\"\"\"\n",
                "        m = y_true.shape[0]\n",
                "        epsilon = 1e-8\n",
                "        loss = -np.mean(y_true * np.log(y_pred + epsilon) + (1 - y_true) * np.log(1 - y_pred + epsilon))\n",
                "        return loss\n",
                "    \n",
                "    def backward(self, y_true, learning_rate=0.01):\n",
                "        \"\"\"Backpropagation â€” compute gradients and update weights.\"\"\"\n",
                "        m = y_true.shape[0]\n",
                "        \n",
                "        # Output layer gradient\n",
                "        delta = self.activations[-1] - y_true  # dL/dz for BCE + sigmoid\n",
                "        \n",
                "        for i in range(self.n_layers - 1, -1, -1):\n",
                "            # Compute gradients\n",
                "            dW = (self.activations[i].T @ delta) / m\n",
                "            db = np.mean(delta, axis=0, keepdims=True)\n",
                "            \n",
                "            # Propagate to previous layer\n",
                "            if i > 0:\n",
                "                delta = (delta @ self.weights[i].T) * relu_derivative(self.z_values[i-1])\n",
                "            \n",
                "            # Update weights\n",
                "            self.weights[i] -= learning_rate * dW\n",
                "            self.biases[i] -= learning_rate * db\n",
                "    \n",
                "    def train(self, X, y, epochs=1000, lr=0.1, print_every=100):\n",
                "        \"\"\"Training loop.\"\"\"\n",
                "        losses = []\n",
                "        \n",
                "        for epoch in range(epochs):\n",
                "            # Forward\n",
                "            y_pred = self.forward(X)\n",
                "            loss = self.compute_loss(y_pred, y)\n",
                "            losses.append(loss)\n",
                "            \n",
                "            # Backward\n",
                "            self.backward(y, learning_rate=lr)\n",
                "            \n",
                "            if (epoch + 1) % print_every == 0:\n",
                "                acc = np.mean((y_pred > 0.5).astype(float) == y)\n",
                "                print(f'Epoch {epoch+1:4d} | Loss: {loss:.4f} | Accuracy: {acc:.2%}')\n",
                "        \n",
                "        return losses\n",
                "\n",
                "# Create and train\n",
                "nn = NeuralNetwork([2, 16, 8, 1])\n",
                "losses = nn.train(X, y, epochs=2000, lr=0.1, print_every=200)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Visualize Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Loss curve\n",
                "axes[0].plot(losses, 'b-', alpha=0.7)\n",
                "axes[0].set_xlabel('Epoch')\n",
                "axes[0].set_ylabel('Loss')\n",
                "axes[0].set_title('Training Loss')\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "# Decision boundary\n",
                "h = 0.05\n",
                "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
                "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
                "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
                "grid = np.column_stack([xx.ravel(), yy.ravel()])\n",
                "\n",
                "Z = nn.forward(grid)\n",
                "Z = Z.reshape(xx.shape)\n",
                "\n",
                "axes[1].contourf(xx, yy, Z, levels=50, cmap='RdBu_r', alpha=0.8)\n",
                "axes[1].scatter(X[y.ravel()==0, 0], X[y.ravel()==0, 1], c='blue', s=20, edgecolors='white', linewidth=0.5)\n",
                "axes[1].scatter(X[y.ravel()==1, 0], X[y.ravel()==1, 1], c='red', s=20, edgecolors='white', linewidth=0.5)\n",
                "axes[1].set_title('Decision Boundary')\n",
                "axes[1].axis('equal')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Final accuracy\n",
                "y_pred = nn.forward(X)\n",
                "final_acc = np.mean((y_pred > 0.5).astype(float) == y)\n",
                "print(f'\\nðŸŽ¯ Final Accuracy: {final_acc:.2%}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ’¡ Key Takeaways\n",
                "\n",
                "| Concept | What We Used | Why |\n",
                "|---------|-------------|-----|\n",
                "| **He Initialization** | `âˆš(2/fan_in)` scaling | Prevents vanishing/exploding gradients |\n",
                "| **ReLU** | Hidden layers | Non-linear, no vanishing gradient, fast |\n",
                "| **Sigmoid** | Output layer | Squashes to [0,1] for binary classification |\n",
                "| **BCE Loss** | `-yÂ·log(Å·) - (1-y)Â·log(1-Å·)` | Standard for binary classification |\n",
                "| **Backprop** | Chain rule through layers | Efficient gradient computation |\n",
                "| **Learning Rate** | 0.1 | Controls step size (too high = diverge, too low = slow) |\n",
                "\n",
                "### Interview Questions This Covers\n",
                "- \"Implement a neural network from scratch\"\n",
                "- \"Explain backpropagation\"\n",
                "- \"Why do we use ReLU over sigmoid in hidden layers?\"\n",
                "- \"What is the vanishing gradient problem?\"\n",
                "- \"Explain He vs Xavier initialization\""
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}